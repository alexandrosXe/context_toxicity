{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfJ3BMHHk6udQT8y+eWR6H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexandrosXe/context_toxicity/blob/master/Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQrllRh18Vlm",
        "colab_type": "text"
      },
      "source": [
        "## **Bert Model for Toxicity Detection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmo-XY2oVdYT",
        "colab_type": "text"
      },
      "source": [
        "# Install pkbar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqstP7s7Vdzi",
        "colab_type": "code",
        "outputId": "2e8cd8fe-4173-4752-8e19-ed0f94929804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install pkbar"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pkbar in /usr/local/lib/python3.6/dist-packages (0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pkbar) (1.18.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QecwaRs8cgD",
        "colab_type": "text"
      },
      "source": [
        "# Setup. Download the  Transformers library by Hugging Face:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf1U1mTy_bCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -qq transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1G2audg9IAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import pkbar\n",
        "from sklearn.metrics import *\n",
        "import torch.autograd\n",
        "from torch.autograd import Variable\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTUZJixq1FhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "#tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0gGXIYMdbci",
        "colab_type": "text"
      },
      "source": [
        "# Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nTcAuzgdd1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStopping:\n",
        "  def __init__(self, patience=7, verbose=False, delta=0):\n",
        "    self.patience = patience\n",
        "    self.verbose = verbose\n",
        "    self.counter = 0\n",
        "    self.best_score = None\n",
        "    self.early_stop = False\n",
        "    self.val_loss_min = np.Inf\n",
        "    self.delta = delta\n",
        "\n",
        "  def __call__(self, val_loss, model):\n",
        "\n",
        "    score = val_loss\n",
        "\n",
        "    if self.best_score is None:\n",
        "      self.best_score = score\n",
        "      self.save_checkpoint(val_loss, model)\n",
        "    elif score <= self.best_score + self.delta:\n",
        "      self.counter += 1\n",
        "      print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "      if self.counter >= self.patience:\n",
        "        self.early_stop = True\n",
        "    else:\n",
        "      self.best_score = score\n",
        "      self.save_checkpoint(val_loss, model)\n",
        "      self.counter = 0\n",
        "\n",
        "  def save_checkpoint(self, val_loss, model):\n",
        "    if self.verbose:\n",
        "      print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "      torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "      self.val_loss_min = val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP5ayBd1Mxlq",
        "colab_type": "text"
      },
      "source": [
        "# Create a PyTorch dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BxsaiuxMyrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Toxic_Detection_Dataset(Dataset):\n",
        "  def __init__(self, comments, targets, tokenizer, max_len):\n",
        "    self.comments = comments\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.comments)\n",
        "\n",
        "    \n",
        "  def __getitem__(self, item):\n",
        "    comment = str(self.comments[item])\n",
        "    target = self.targets[item]\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      comment,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "    return {\n",
        "      'comment_text': comment,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4D6_kmNHqsx",
        "colab_type": "text"
      },
      "source": [
        "# **Bert MLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG_intA0D4xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERT_MLP(nn.Module):\n",
        "  def __init__(self,\n",
        "                 trainable_layers=3,\n",
        "                 max_seq_length=128,\n",
        "                 show_summary=False,\n",
        "                 label_list=[0, 1],\n",
        "                 patience=3,\n",
        "                 seed=42,\n",
        "                 epochs=100,\n",
        "                 save_predictions=False,\n",
        "                 batch_size=32,\n",
        "                 DATA_COLUMN=\"text\",\n",
        "                 LABEL_COLUMN=\"label\",\n",
        "                 DATA2_COLUMN=None,\n",
        "                 lr=2e-05,\n",
        "                 session=None,\n",
        "                 loss=nn.BCELoss()\n",
        "                 ):\n",
        "    super(BERT_MLP, self).__init__()\n",
        "    self.name = f'{\"OOC1\" if not DATA2_COLUMN else \"OOC2\"}-b{batch_size}.e{epochs}.len{max_seq_length}.bert'\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.lr = lr\n",
        "    self.batch_size = batch_size\n",
        "    self.DATA_COLUMN=DATA_COLUMN\n",
        "    self.DATA2_COLUMN=DATA2_COLUMN\n",
        "    self.LABEL_COLUMN=LABEL_COLUMN\n",
        "    self.trainable_layers = trainable_layers\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.show_summary = show_summary\n",
        "    self.label_list = label_list\n",
        "    self.patience=patience\n",
        "    self.save_predictions = save_predictions\n",
        "    self.epochs = epochs\n",
        "    self.loss = loss\n",
        "\n",
        "    #Layers\n",
        "    self.bert = BertModel.from_pretrained('bert-base-cased') # (PRE_TRAINED_MODEL_NAME)\n",
        "    self.dense = nn.Linear(self.bert.config.hidden_size,128)\n",
        "    #self.denseBn = nn.BatchNorm1d(128)\n",
        "    self.tanh=nn.Tanh()\n",
        "    self.output = nn.Linear(128,1)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "\n",
        "    #if possible run in GPU else in CPU\n",
        "    if torch.cuda.is_available():\n",
        "      self.device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
        "      print(\"Running on the GPU\")\n",
        "    else:\n",
        "      self.device = torch.device(\"cpu\")\n",
        "      print(\"Running on the CPU\")\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    # Get output from bert decoder \n",
        "    cont_reps, pooled_output = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
        "    #cls_rep = cont_reps[:, 0]\n",
        "    # feed it to dense NN \n",
        "    #output = self.dense(cls_rep)\n",
        "    output = self.dense(pooled_output)\n",
        "    #output=self.densebn(output)\n",
        "    output = self.tanh(output)\n",
        "    output = self.output(output)\n",
        "    output = self.sigmoid(output)\n",
        "    #print(output)\n",
        "    return output.squeeze()\n",
        "\n",
        "  def fit(self, train, val,optimizer,bert_weights=None): #pretrained_embeddings\n",
        "    #Counter class imbalance by setting output layer bias to log(T/N)\n",
        "    pos = sum(train.label)\n",
        "    neg = len(train.label)-pos\n",
        "    bias = np.log(pos/neg)\n",
        "    print (\"BIAS:\", bias)\n",
        "    bias=torch.tensor(bias)\n",
        "    with torch.no_grad():\n",
        "      self.output.bias=torch.nn.Parameter(bias.to(torch.float)) # set bias of last dense layer log(T/N)\n",
        "    self.output.bias.requires_grad_(False)\n",
        "    early_stopping = EarlyStopping(patience=self.patience, verbose=True)     #EARLYY STOPING\n",
        "    self.to(self.device)\n",
        "    i=1 #for progress bar\n",
        "    for epoch in range(self.epochs):\n",
        "      print('\\nEpoch: %d/%d' % (epoch + 1, self.epochs))\n",
        "      kbar = pkbar.Kbar(target=self.epochs, width=10)\n",
        "      kbar_val=pkbar.Kbar(target=self.epochs, width=10)\n",
        "      epoch_loss,epoch_Auc,epoch_accuracy=self.trainin(train,optimizer)\n",
        "      kbar.update(i, values=[(\"loss\",epoch_loss), (\"accuracy\",epoch_accuracy),(\"AUC_score\",epoch_Auc)])  #(\"precision\",epoch_precision),(\"recall\",epoch_recall),(\"F1\",epoch_F1)])\n",
        "      # val_loss,val_AUC_score,val_accuracy,val_recall,val_precision,val_F1=self.evaluate(val)\n",
        "      val_loss,val_AUC_score,val_accuracy=self.evaluate(val)\n",
        "      print(\"\\n Val auc score in epoch \",epoch+1, \":\",val_AUC_score)\n",
        "      early_stopping(val_AUC_score,self)   \n",
        "      if early_stopping.early_stop: #check for early stopping\n",
        "        print(\"Early stopping\")\n",
        "        print(\"_________________________________________________-\")\n",
        "        break\n",
        "      kbar_val.update(i,values=[(\"val_loss\",val_loss), (\"val_accuracy\",val_accuracy),(\"val_AUC_score\",val_AUC_score)])#,(\"val_precision\",val_precision),(\"val_recall\",val_recall),(\"val_F1\",val_F1)])\n",
        "      i+=1\n",
        "\n",
        "  def trainin(self,train,optimizer):\n",
        "    #Create Data Loader for mini batch Training\n",
        "\n",
        "    #UnderSampling\n",
        "    majority_weight = 1/(6000-59)\n",
        "    minority_weight = 1/59\n",
        "    sample_weights = np.array([majority_weight, minority_weight])\n",
        "    weights = sample_weights[train.label]\n",
        "    sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
        "    train_ds = Toxic_Detection_Dataset(comments=train.text.to_numpy(),targets=train.label.to_numpy(),tokenizer=self.tokenizer,max_len=self.max_seq_length)\n",
        "    # train_dl=DataLoader(train_ds,batch_size=self.batch_size,shuffle=True)\n",
        "    train_dl=DataLoader(train_ds,batch_size=self.batch_size,sampler=sampler)\n",
        "    epoch_loss=0\n",
        "    correct_predictions=0\n",
        "    Y=torch.empty(0) #create empty torch to append predictions\n",
        "    self.train()\n",
        "    for d in train_dl:\n",
        "      #Get Bert inputs \n",
        "      input_ids = d[\"input_ids\"].to(self.device)\n",
        "      attention_mask = d[\"attention_mask\"].to(self.device)\n",
        "      targets = d[\"targets\"].to(self.device)\n",
        "      outputs = self(input_ids=input_ids,attention_mask=attention_mask)\n",
        "      weight = torch.tensor([0.1, 0.9]).to(self.device)\n",
        "      weight_ = weight[targets.data.view(-1).long()].view_as(targets)\n",
        "      criterion = nn.BCELoss(reduction='none')  #(reduce=False)\n",
        "      loss = criterion(outputs, targets.to(torch.float))\n",
        "      loss_class_weighted = loss * weight_\n",
        "      loss_class_weighted = loss_class_weighted.mean()\n",
        "      loss=loss_class_weighted\n",
        "      #loss = self.loss(outputs, targets.to(torch.float))\n",
        "      epoch_loss+=loss.item()\n",
        "      #losses.append(loss.item())\n",
        "      Y=torch.cat((Y,outputs.cpu()),0)\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "    with torch.no_grad():\n",
        "      predictions=Y\n",
        "      rounded=torch.round(predictions).cpu().detach().numpy()\n",
        "      predictions=predictions.cpu().detach().numpy()\n",
        "      epoch_Auc=roc_auc_score(train.label.to_numpy(),predictions)\n",
        "      epoch_Accuracy=accuracy_score(train.label.to_numpy(),rounded)\n",
        "    return epoch_loss,epoch_Auc,epoch_Accuracy\n",
        "  \n",
        "  #Validation\n",
        "  def evaluate(self,val):\n",
        "    val_ds = Toxic_Detection_Dataset(comments=val.text.to_numpy(),targets=val.label.to_numpy(),tokenizer=self.tokenizer,max_len=self.max_seq_length)\n",
        "    val_dl=DataLoader(val_ds,batch_size=self.batch_size,shuffle=True)\n",
        "    val_loss=0\n",
        "    correct_predictions=0\n",
        "    Y=torch.empty(0) #create empty torch to append predictions\n",
        "    self.eval()\n",
        "    with torch.no_grad():   # compute validation loss\n",
        "      for d in val_dl:\n",
        "        input_ids = d[\"input_ids\"].to(self.device)\n",
        "        attention_mask = d[\"attention_mask\"].to(self.device)\n",
        "        targets = d[\"targets\"].to(self.device)\n",
        "        outputs = self(input_ids=input_ids,attention_mask=attention_mask)\n",
        "        weight = torch.tensor([0.1, 0.9]).to(self.device)\n",
        "        weight_ = weight[targets.data.view(-1).long()].view_as(targets)\n",
        "        criterion = nn.BCELoss(reduction='none')  #(reduce=False)\n",
        "        loss = criterion(outputs, targets.to(torch.float))\n",
        "        loss_class_weighted = loss * weight_\n",
        "        loss_class_weighted = loss_class_weighted.mean()\n",
        "        loss=loss_class_weighted\n",
        "        Y=torch.cat((Y,outputs.cpu()),0)\n",
        "        #loss = self.loss(outputs, targets.to(torch.float))\n",
        "        val_loss+=loss.item()\n",
        "    predictions=Y\n",
        "    rounded=torch.round(predictions).cpu().detach().numpy()\n",
        "    predictions=predictions.cpu().detach().numpy()\n",
        "    val_Auc=roc_auc_score(val.label.to_numpy(),predictions)\n",
        "    val_Accuracy=accuracy_score(val.label.to_numpy(),rounded)\n",
        "    return val_loss,val_Auc,val_Accuracy\n",
        "\n",
        "  #predict on test data\n",
        "  def predict(self,test):\n",
        "    test_ds = Toxic_Detection_Dataset(comments=test.text.to_numpy(),targets=test.label.to_numpy(),tokenizer=self.tokenizer,max_len=self.max_seq_length)\n",
        "    test_dl=DataLoader(test_ds,batch_size=self.batch_size,shuffle=True)\n",
        "    Y=torch.empty(0) #create empty torch to append predictions\n",
        "    self.eval()\n",
        "    with torch.no_grad():   # compute validation loss\n",
        "      for d in test_dl:\n",
        "        input_ids = d[\"input_ids\"].to(self.device)\n",
        "        attention_mask = d[\"attention_mask\"].to(self.device)\n",
        "        targets = d[\"targets\"].to(self.device)\n",
        "        outputs = self(input_ids=input_ids,attention_mask=attention_mask)\n",
        "        Y=torch.cat((Y,outputs.cpu()),0)\n",
        "    predictions=Y\n",
        "    return predictions \n",
        "\n",
        "  def Unfreeze_Last_K_Layers(self,k=3):\n",
        "    ct = 0\n",
        "    #Bert layers\n",
        "    for child in self.bert.children():\n",
        "      ct += 1\n",
        "      if (ct <= (12-k)):\n",
        "        #Bert's layer's parameters\n",
        "        for param in child.parameters():\n",
        "          param.requires_grad = False\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvO8rcOGaowK",
        "colab_type": "text"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzgebU4eav79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataN=pd.read_csv(\"dataset/oc.csv\",header=0)\n",
        "dataC=pd.read_csv(\"dataset/wc.csv\",header=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCI2rfXoujOK",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIDjXKWYbHrk",
        "colab_type": "text"
      },
      "source": [
        "# 5-fold MC Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7xCQnEJbOHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MC_Validation(dataset,k=5):\n",
        "  avgscore=0\n",
        "  for i in range(k):\n",
        "    model= BERT_MLP(epochs=20)\n",
        "\n",
        "    optimizer=optim.Adam(model.parameters(), lr=model.lr)\n",
        "\n",
        "    # train @N models\n",
        "    X_train, X_test = train_test_split(dataset, test_size=0.4,random_state=i)\n",
        "    #X_train, X_val = train_test_split(X_train,test_size=0.25,random_state=i) # 0.25 x 0.8 = 0.2\n",
        "    XC_train,X_test=train_test_split(dataC, test_size=0.2,random_state=i) # test dataset with C to train and test\n",
        "    XC_train,X_val= train_test_split(XC_train,test_size=0.25,random_state=i) # 0.25 x 0.8 = 0.2\n",
        "\n",
        "    # train @C models\n",
        "    # X_train, X_test = train_test_split(dataset, test_size=0.4,random_state=i)\n",
        "    # #X_train, X_val = train_test_split(X_train,test_size=0.25,random_state=i) # 0.25 x 0.8 = 0.2\n",
        "    # X_val,X_test=train_test_split(X_test, test_size=0.5,random_state=i) # test dataset with C to train and test\n",
        "\n",
        "    # Perform MC Validation\n",
        "    model.fit(X_train,X_val,optimizer)  #bert_weights=\"bert_weights.h5\"\n",
        "    preds=model.predict(X_test)#X_test.text.to_numpy())\n",
        "    preds=preds.cpu().detach().numpy()\n",
        "    gold=X_test.label\n",
        "    print(\"\\n__________________________________\\n\") \n",
        "    score = roc_auc_score(gold, preds)\n",
        "    print(\"AUC score in \",i+1,\" fold \",score)\n",
        "    print(\"\\n__________________________________\") #to see results\n",
        "    avgscore+=score\n",
        "  avgscore/=k\n",
        "  return avgscore"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ziYIxKSbryH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Average AUC score over 5fold MC validation is \",MC_Validation(dataset=dataN,k=5)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfW-tJl8_7AI",
        "colab_type": "text"
      },
      "source": [
        "# Stratified MC validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu5_UnLx_7sc",
        "colab_type": "code",
        "outputId": "0709864f-4d7c-48af-af5d-298e933ebbb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Make stratified split in @N dataset for train data\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.4, random_state=0)\n",
        "X=dataN.text.to_numpy()\n",
        "y=dataN.label.to_numpy()\n",
        "train=list(sss.split(X,y))\n",
        "\n",
        "#Make stratified split in @C dataset for val and test data\n",
        "sss_Val = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=3)\n",
        "X_val=dataC.text.to_numpy()\n",
        "y_val=dataC.label.to_numpy()\n",
        "val_and_test=list(sss_Val.split(X_val,y_val))\n",
        "train_index,test_index = val_and_test[0]\n",
        "X_train, X_val_test = X_val[train_index], X_val[test_index]\n",
        "y_train, y_val_test = y_val[train_index], y_val[test_index]\n",
        "\n",
        "# Now make stratified split in 40% of C dataset for val(20%) and test(20%)\n",
        "sss_Val= StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
        "val_and_test=list( sss_Val.split(X_val_test,y_val_test))\n",
        "avgscore=0\n",
        "for i in range(5):\n",
        "  train_index, _ = train[i]\n",
        "  val_index , test_index = val_and_test[i]\n",
        "  X_train,y_train=X[train_index],y[train_index]\n",
        "  X_val,y_val=X_val_test[val_index],y_val_test[val_index]\n",
        "  X_test,y_test=X_val_test[test_index],y_val_test[test_index]\n",
        "  \n",
        "  # Create X_train , X_val and X_test Dataframes\n",
        "  X_train=pd.DataFrame({'text': X_train,'label': y_train})\n",
        "  X_val=pd.DataFrame({'text': X_val,'label': y_val})\n",
        "  X_test=pd.DataFrame({'text': X_test,'label': y_test})\n",
        "  #print(X_train.loc[X_train['label']==1].shape)\n",
        "  # print(X_val.shape)\n",
        "  # print(X_test.shape)\n",
        "  model= BERT_MLP(epochs=20)\n",
        "  model.Unfreeze_Last_K_Layers(k=3)\n",
        "  optimizer=optim.Adam(model.parameters(), lr=model.lr)\n",
        "  model.fit(X_train,X_val,optimizer)  #bert_weights=\"bert_weights.h5\"\n",
        "  preds=model.predict(X_test)\n",
        "  preds=preds.cpu().detach().numpy()\n",
        "  gold=X_test.label.to_numpy()\n",
        "  print(\"\\n__________________________________\\n\") \n",
        "  score = roc_auc_score(gold, preds)\n",
        "  print(\"AUC score in \",i+1,\" fold \",score)\n",
        "  print(\"\\n__________________________________\") #to see results\n",
        "  avgscore+=score\n",
        "result=avgscore/5\n",
        "print(\"Average AUC score over 5fold MC validation is \",result) "
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n",
            "BIAS: -5.138316273042602\n",
            "\n",
            "Epoch: 1/20\n",
            " 1/20 [..........] - ETA: 28:12 - loss: 135.7944 - accuracy: 0.9797 - AUC_score: 0.4942\n",
            " Val auc score in epoch  1 : 0.5787478849407783\n",
            "Validation loss decreased (inf --> 0.578748).  Saving model ...\n",
            " 1/20 [..........] - ETA: 37:42 - val_loss: 4.9042 - val_accuracy: 0.3925 - val_AUC_score: 0.5787\n",
            "Epoch: 2/20\n",
            " 2/20 [>.........] - ETA: 13:18 - loss: 41.3832 - accuracy: 0.0280 - AUC_score: 0.5108\n",
            " Val auc score in epoch  2 : 0.542258883248731\n",
            "EarlyStopping counter: 1 out of 3\n",
            " 2/20 [>.........] - ETA: 17:37 - val_loss: 7.8768 - val_accuracy: 0.0150 - val_AUC_score: 0.5423\n",
            "Epoch: 3/20\n",
            " 3/20 [>.........] - ETA: 8:21 - loss: 34.8376 - accuracy: 0.0058 - AUC_score: 0.5010\n",
            " Val auc score in epoch  3 : 0.46881556683587144\n",
            "EarlyStopping counter: 2 out of 3\n",
            " 3/20 [>.........] - ETA: 11:04 - val_loss: 9.5162 - val_accuracy: 0.0150 - val_AUC_score: 0.4688\n",
            "Epoch: 4/20\n",
            " 4/20 [=>........] - ETA: 5:54 - loss: 33.1883 - accuracy: 0.0058 - AUC_score: 0.4905\n",
            " Val auc score in epoch  4 : 0.5574619289340101\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            "_________________________________________________-\n",
            "\n",
            "__________________________________\n",
            "\n",
            "AUC score in  1  fold  0.4437817258883249\n",
            "\n",
            "__________________________________\n",
            "Running on the GPU\n",
            "BIAS: -5.138316273042602\n",
            "\n",
            "Epoch: 1/20\n",
            " 1/20 [..........] - ETA: 28:06 - loss: 135.5241 - accuracy: 0.9165 - AUC_score: 0.5887\n",
            " Val auc score in epoch  1 : 0.5007021996615906\n",
            "Validation loss decreased (inf --> 0.500702).  Saving model ...\n",
            " 1/20 [..........] - ETA: 37:31 - val_loss: 5.3631 - val_accuracy: 0.0365 - val_AUC_score: 0.5007\n",
            "Epoch: 2/20\n",
            " 2/20 [>.........] - ETA: 13:18 - loss: 39.8806 - accuracy: 0.0090 - AUC_score: 0.5604"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-36b7b2c3cc1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnfreeze_Last_K_Layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#bert_weights=\"bert_weights.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-fecc0e08fcb7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train, val, optimizer, bert_weights)\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0mkbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC_score\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_Auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#(\"precision\",epoch_precision),(\"recall\",epoch_recall),(\"F1\",epoch_F1)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0;31m# val_loss,val_AUC_score,val_accuracy,val_recall,val_precision,val_F1=self.evaluate(val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m       \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_AUC_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n Val auc score in epoch \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_AUC_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0mearly_stopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_AUC_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-fecc0e08fcb7>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mweight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#(reduce=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mBj7DpeLCAy",
        "colab_type": "code",
        "outputId": "70dd6eb9-81db-451f-af2f-612b66265984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Get toxic comments and give them always to train set\n",
        "toxics=dataN.loc[dataN['label']==1]\n",
        "Non_toxics=dataN[dataN.label != 1]\n",
        "Non_toxics=dataN[0:59]\n",
        "#print(Non_toxics.shape)\n",
        "# Non_toxics=dataN.loc[dataN['label']==0]\n",
        "# Non_toxics=Non_toxics[0:100]\n",
        "# #print(Non_toxics.label.head(100))\n",
        "frames = [toxics,Non_toxics]\n",
        "data = pd.concat(frames)\n",
        "print(data.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(118, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYiv8inSLauE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avgscore=0\n",
        "for i in range(5):\n",
        "  model=BERT_MLP(epochs=20)\n",
        "  model.Unfreeze_Last_K_Layers(k=3)\n",
        "  optimizer=optim.Adam(model.parameters(), lr=model.lr)\n",
        "  X_train, X_val = train_test_split(data, test_size=0.2,random_state=i)\n",
        "  model.fit(data,X_val,optimizer)  #bert_weights=\"bert_weights.h5\"\n",
        "  preds=model.predict(data)\n",
        "  preds=preds.cpu().detach().numpy()\n",
        "  #preds=torch.round(preds).cpu().detach().numpy()\n",
        "  gold=data.label.to_numpy()\n",
        "  print(\"\\n__________________________________\\n\") \n",
        "  score = roc_auc_score(gold, preds)\n",
        "  print(\"AUC score in \",i+1,\" fold \",score)\n",
        "  print(\"\\n__________________________________\") #to see results\n",
        "  avgscore+=score\n",
        "result=avgscore/5\n",
        "print(\"Average AUC score over 5fold MC validation is \",result) "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}