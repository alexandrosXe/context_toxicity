{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BILSTMS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexandrosXe/context_toxicity/blob/master/BILSTMS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDukPC07mtZw",
        "colab_type": "text"
      },
      "source": [
        "# **Pytorch Implemantation for Context Toxicity**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXwlY7PW2eRb",
        "colab_type": "text"
      },
      "source": [
        "# Install Progress bar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEIdCyl72jBr",
        "colab_type": "code",
        "outputId": "b84fb712-34e9-483d-8037-9c6a1d90a809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install pkbar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pkbar\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/43/367098af2862f7b4e6aa871494ed3d66c57af849a5962a92baa2dd99b652/pkbar-0.4-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pkbar) (1.18.4)\n",
            "Installing collected packages: pkbar\n",
            "Successfully installed pkbar-0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l554tarpmpER",
        "colab_type": "text"
      },
      "source": [
        "# Classifiers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjCVaXpUm3p_",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RM0X3PQwaSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torchtext\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.metrics import *\n",
        "import pkbar\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import  DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import torch.autograd\n",
        "from torch.autograd import Variable\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from torch.utils.data import WeightedRandomSampler\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcMWjbUoBaRm",
        "colab_type": "text"
      },
      "source": [
        "# **Load Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbvNeCSfBdkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_embeddings_index():\n",
        "  embeddings_index = dict()\n",
        "  with open('embeddings/glove.6B.100d.txt', 'r') as glove_in:\n",
        "    for line in glove_in.readlines():\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  return embeddings_index\n",
        "\n",
        "def Compute_Vocab_Size(train,augmented_vocabulary=False):\n",
        "  texts = train.text if not augmented_vocabulary else train.text + train.parent\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "  print('Vocabulary Size: %d' % vocab_size)\n",
        "  return vocab_size\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt51CPDNzD_G",
        "colab_type": "text"
      },
      "source": [
        "# **Early Stoping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzHz3kZRzGUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStopping:\n",
        "  def __init__(self, patience=7, verbose=False, delta=0):\n",
        "    self.patience = patience\n",
        "    self.verbose = verbose\n",
        "    self.counter = 0\n",
        "    self.best_score = None\n",
        "    self.early_stop = False\n",
        "    self.val_loss_min = np.Inf\n",
        "    self.delta = delta\n",
        "\n",
        "  def __call__(self, val_loss, model):\n",
        "\n",
        "    score = val_loss\n",
        "\n",
        "    if self.best_score is None:\n",
        "      self.best_score = score\n",
        "      self.save_checkpoint(val_loss, model)\n",
        "    elif score <= self.best_score + self.delta:\n",
        "      self.counter += 1\n",
        "      print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "      if self.counter >= self.patience:\n",
        "        self.early_stop = True\n",
        "    else:\n",
        "      self.best_score = score\n",
        "      self.save_checkpoint(val_loss, model)\n",
        "      self.counter = 0\n",
        "\n",
        "  def save_checkpoint(self, val_loss, model):\n",
        "    if self.verbose:\n",
        "      print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "      torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "      self.val_loss_min = val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yvT6PLcm6Zm",
        "colab_type": "text"
      },
      "source": [
        "# LSTM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ-5JF4AnLvl",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "\n",
        "class LSTM_CLF(nn.Module):\n",
        "  def __init__(self,vocab_size ,stacks=0, verbose=1, batch_size=128, n_epochs=100, max_length=512,\n",
        "                 loss=nn.BCELoss(),monitor_loss=\"val_loss\",patience=3, \n",
        "                 prefix=\"vanilla\",\n",
        "                 hidden_size=128,\n",
        "                 word_embedding_size=200,\n",
        "                 seed=42,\n",
        "                 augmented_vocabulary = False,\n",
        "                 no_sigmoid=False):\n",
        "    super(LSTM_CLF, self).__init__()\n",
        "    np.random.seed(seed)\n",
        "    self.verbose = verbose\n",
        "    self.augmented_vocabulary = augmented_vocabulary\n",
        "    self.patience = patience\n",
        "    self.batch_size = batch_size\n",
        "    self.n_epochs = n_epochs\n",
        "    self.no_sigmoid = no_sigmoid\n",
        "    self.stacks=stacks\n",
        "    self.max_length = max_length\n",
        "    self.tokenizer = Tokenizer()\n",
        "    self.loss = loss\n",
        "    self.word_embedding_size = word_embedding_size\n",
        "    self.hidden_size=hidden_size\n",
        "    self.prefix = prefix\n",
        "    self.monitor_loss = monitor_loss\n",
        "    self.name = f'{prefix}-b{batch_size}.e{n_epochs}.len{max_length}.rnn'\n",
        "    self.vocab_size=vocab_size\n",
        "\n",
        "    #Layers\n",
        "    self.word_embeds = nn.Embedding(self.vocab_size+2,self.word_embedding_size,padding_idx=0)\n",
        "    #self.embeds_bn=nn.BatchNorm1d(self.max_length)\n",
        "    if(stacks!=0):\n",
        "      self.stacked_lstm = nn.LSTM(self.word_embedding_size,self.hidden_size,num_layers=self.stacks,bidirectional=True)\n",
        "      self.dense=nn.Linear(self.hidden_size*2*self.stacks,128)\n",
        "    else:\n",
        "      self.stacked_lstm = nn.LSTM(self.word_embedding_size,self.hidden_size,bidirectional=True)\n",
        "      self.lstm_bn = nn.BatchNorm1d(2*self.hidden_size)\n",
        "      self.lstm_ln = nn.LayerNorm(2*self.hidden_size,elementwise_affine=False)\n",
        "      self.dense=nn.Linear(2*self.hidden_size,128)\n",
        "      self.dense1_bn = nn.BatchNorm1d(128)\n",
        "      #self.dense1_ln = nn.LayerNorm(self.hidden_size,elementwise_affine=False)\n",
        "    self.out=nn.Linear(128,1)\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "\n",
        "  def init_weights(self,bias):\n",
        "        initrange = 0.5\n",
        "        self.word_embeds.weight.data.uniform_(-initrange, initrange)\n",
        "        #self.stacked_lstm.weight.data.uniform_(-initrange, initrange)\n",
        "        self.dense.weight.data.uniform_(-initrange, initrange)\n",
        "        self.dense.bias.data.zero_()\n",
        "        self.out.weight.data.uniform_(-initrange, initrange)\n",
        "        self.out.bias.data=bias\n",
        "        \n",
        "\n",
        "  def init_hidden(self,batch_size):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # Refer to the Pytorch documentation to see exactly\n",
        "        # why they have this dimensionality.\n",
        "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
        "        return (Variable(torch.zeros(2, batch_size, self.hidden_size)).cuda(),   \n",
        "                Variable(torch.zeros(2, batch_size, self.hidden_size).cuda()))    # <- change here: first dim of hidden needs to be doubled\n",
        "\n",
        "  def forward(self, Text):\n",
        "    hidden=self.init_hidden(len(Text))\n",
        "    embeds=self.word_embeds(Text.to(torch.long))\n",
        "    #embeds=self.embeds_bn(embeds)\n",
        "    stacks, hidden =self.stacked_lstm(embeds.view(self.max_length,len(Text),self.word_embedding_size),hidden)\n",
        "    concatenated=stacks.squeeze()[-1,:]\n",
        "    #concatenated=self.lstm_ln(concatenated)\n",
        "    output=self.dense(concatenated)\n",
        "    #output=self.dense1_bn(output)\n",
        "    output=self.tanh(output)\n",
        "    output=self.out(output)\n",
        "    output=self.sigmoid(output)\n",
        "    #print(output)\n",
        "    return output.to(torch.float)\n",
        "\n",
        "  def model_show(self):\n",
        "    print(self)\n",
        "\n",
        "  def load_embeddings(self, pretrained_dict):\n",
        "    self.embedding_matrix = np.zeros((self.vocab_size + 2, 100))\n",
        "    for word, index in self.tokenizer.word_index.items():\n",
        "      embedding_vector = pretrained_dict.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        self.embedding_matrix[index + 1] = embedding_vector\n",
        "    return torch.from_numpy(self.embedding_matrix)\n",
        "\n",
        "  \n",
        "  def text_process(self, texts):\n",
        "    x1 = self.tokenizer.texts_to_sequences(texts.to_numpy())\n",
        "    x1 = sequence.pad_sequences(x1, maxlen=self.max_length)  # padding\n",
        "    return x1\n",
        "    #return torch.tensor(x1, dtype=torch.long)\n",
        "  \n",
        "\n",
        "  def trainin(self,X_train,Y_train, optimizer,device):\n",
        "    #UnderSampling\n",
        "    majority_weight = 1/(6000-59)\n",
        "    minority_weight = 1/59\n",
        "    sample_weights = np.array([majority_weight, minority_weight])\n",
        "    weights = sample_weights[Y_train]\n",
        "    sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
        "    train_ds = TensorDataset(X_train,Y_train)\n",
        "    train_dl = DataLoader(train_ds, batch_size=self.batch_size, sampler=sampler)\n",
        "    # train_dl = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n",
        "    epoch_loss=0 \n",
        "    epoch_Auc=0\n",
        "    epoch_accuracy=0\n",
        "    epoch_recall=0\n",
        "    epoch_precision=0\n",
        "    epoch_F1=0\n",
        "    Y=torch.empty(0) #create empty torch to append predictions\n",
        "    self.train()\n",
        "    for xb, yb in train_dl:\n",
        "      xb, yb = xb.to(device), yb.to(device)  #move batches to GPU or CPU\n",
        "      # self.zero_grad() #DES TO\n",
        "      predictions =self(xb)  #run model for mini batch\n",
        "      predictions=predictions.squeeze()\n",
        "      weight = torch.tensor([0.1, 0.9]).to(device)\n",
        "      weight_ = weight[yb.data.view(-1).long()].view_as(yb)\n",
        "      criterion = nn.BCELoss(reduction='none')   #(reduce=False)\n",
        "      loss = criterion(predictions, yb.to(torch.float))\n",
        "      loss_class_weighted = loss * weight_\n",
        "      loss_class_weighted = loss_class_weighted.mean()\n",
        "      #loss=self.loss(predictions,yb.to(torch.float))#compute loss of mini batch\n",
        "      #epoch_loss+=loss.item()\n",
        "      epoch_loss+=loss_class_weighted.item()\n",
        "      Y=torch.cat((Y,predictions.cpu()),0)\n",
        "      loss_class_weighted.backward() #compute gradients\n",
        "      # loss.backward() #compute gradients\n",
        "      optimizer.step() #update parameters\n",
        "      optimizer.zero_grad() \n",
        "    with torch.no_grad():\n",
        "      predictions=Y\n",
        "      rounded=torch.round(predictions).squeeze().cpu().detach().numpy()\n",
        "      predictions=predictions.cpu().detach().numpy()\n",
        "      epoch_Auc=roc_auc_score(Y_train.cpu().numpy(),predictions)\n",
        "      epoch_accuracy=accuracy_score(Y_train.cpu().numpy(),rounded)\n",
        "      tn, fp, fn, tp =confusion_matrix(Y_train.cpu().numpy(),rounded,labels=[0,1]).ravel()\n",
        "      epoch_precision+=tp/(tp+fp)\n",
        "      epoch_recall+=tp/(tp+fn)\n",
        "      epoch_F1+=2*((epoch_precision*epoch_recall)/(epoch_precision+epoch_recall))\n",
        "    return epoch_loss,epoch_Auc,epoch_accuracy,epoch_recall,epoch_precision,epoch_F1\n",
        "    #return epoch_loss,epoch_Auc,epoch_accuracy\n",
        "  \n",
        "\n",
        "  def evaluate(self,X_dev,Y_dev,device):\n",
        "    dev_ds = TensorDataset(X_dev,Y_dev)\n",
        "    dev_dl = DataLoader(dev_ds, batch_size=self.batch_size)\n",
        "    epoch_loss=0\n",
        "    epoch_Auc=0\n",
        "    epoch_accuracy=0\n",
        "    epoch_recall=0\n",
        "    epoch_precision=0\n",
        "    epoch_F1=0\n",
        "    self.eval()\n",
        "    with torch.no_grad():   # compute validation loss\n",
        "      for xb, yb in dev_dl:\n",
        "        xb,yb=xb.to(device), yb.to(device) #move to Gpu\n",
        "        predictions =self(xb)\n",
        "        weight = torch.tensor([0.1, 0.9]).to(device)\n",
        "        weight_ = weight[yb.data.view(-1).long()].view_as(yb)\n",
        "        criterion = nn.BCELoss(reduction='none')  #(reduce=False)\n",
        "        loss = criterion(predictions, yb.to(torch.float))\n",
        "        loss_class_weighted = loss * weight_\n",
        "        loss_class_weighted = loss_class_weighted.mean()\n",
        "        #val_loss=self.loss(predictions.squeeze(), yb.to(torch.float))\n",
        "        val_loss=loss_class_weighted\n",
        "        epoch_loss+=val_loss\n",
        "      predictions=self(X_dev.to(device))\n",
        "      rounded=torch.round(predictions).squeeze().cpu().detach().numpy()\n",
        "      predictions=predictions.cpu().detach().numpy()\n",
        "      epoch_Auc=roc_auc_score(Y_dev.cpu().numpy(),predictions)\n",
        "      epoch_accuracy=accuracy_score(Y_dev.cpu().numpy(),rounded)\n",
        "      tn, fp, fn, tp =confusion_matrix(Y_dev.cpu().numpy(),rounded,labels=[0,1]).ravel()\n",
        "      epoch_precision+=tp/(tp+fp)\n",
        "      epoch_recall+=tp/(tp+fn)\n",
        "      epoch_F1+=2*((epoch_precision*epoch_recall)/(epoch_precision+epoch_recall))\n",
        "      return epoch_loss,epoch_Auc,epoch_accuracy,epoch_recall,epoch_precision,epoch_F1\n",
        "      #print(epoch_loss,epoch_Auc,epoch_accuracy)\n",
        "      #return epoch_loss,epoch_Auc,epoch_accuracy\n",
        "\n",
        "  \n",
        "  def fit(self, train, dev,optimizer, pretrained_embeddings, class_weights={0: 1, 1: 1}):\n",
        "    np.seterr(over='raise')\n",
        "    texts = train.text if not self.augmented_vocabulary else train.text + train.parent\n",
        "    self.tokenizer.fit_on_texts(texts)\n",
        "    self.vocab_size = len(self.tokenizer.word_index) + 1\n",
        "    print('Vocabulary Size: %d' % self.vocab_size)\n",
        "    pos = sum(train.label)\n",
        "    neg = len(train.label)-pos\n",
        "    bias = np.log(pos/neg)\n",
        "    print(bias,\"BIASSSS\")\n",
        "    bias=torch.tensor(bias)\n",
        "    #self.init_weights(bias.to(torch.float))\n",
        "    with torch.no_grad():\n",
        "      self.out.bias=torch.nn.Parameter(bias.to(torch.float)) # set bias of last dense layer log(T/N)\n",
        "    self.out.bias.requires_grad_(False)\n",
        "    X_train=self.text_process(train.text)\n",
        "    X_train=torch.from_numpy(X_train)\n",
        "    Y_train=torch.from_numpy(train.label.to_numpy())\n",
        "    X_dev=self.text_process(dev.text)\n",
        "    X_dev=torch.from_numpy(X_dev)\n",
        "    Y_dev=torch.from_numpy(dev.label.to_numpy())\n",
        "    early_stopping = EarlyStopping(patience=self.patience, verbose=True)     #EARLYY STOPING\n",
        "    i=1 #for progress bar\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
        "      print(\"Running on the GPU\")\n",
        "    else:\n",
        "      device = torch.device(\"cpu\")\n",
        "      print(\"Running on the CPU\")\n",
        "    self.to(device)\n",
        "    for epoch in range(self.n_epochs):\n",
        "      print('\\nEpoch: %d/%d' % (epoch + 1, self.n_epochs))\n",
        "      kbar = pkbar.Kbar(target=self.n_epochs, width=10)\n",
        "      kbar_val=pkbar.Kbar(target=self.n_epochs, width=10)\n",
        "      epoch_loss,epoch_Auc,epoch_accuracy,epoch_recall,epoch_precision,epoch_F1=self.trainin(X_train,Y_train,optimizer,device)\n",
        "      kbar.update(i, values=[(\"loss\",epoch_loss), (\"accuracy\",epoch_accuracy),(\"AUC_score\",epoch_Auc),(\"precision\",epoch_precision),(\"recall\",epoch_recall),(\"F1\",epoch_F1)])\n",
        "      val_loss,val_AUC_score,val_accuracy,val_recall,val_precision,val_F1=self.evaluate(X_dev,Y_dev,device)\n",
        "      print(\"Val auc score in epoch \",epoch+1, \":\",val_AUC_score)\n",
        "      early_stopping(val_AUC_score,self)   \n",
        "      if early_stopping.early_stop: #check for early stopping\n",
        "        print(\"Early stopping\")\n",
        "        print(\"_________________________________________________-\")\n",
        "        break\n",
        "      kbar_val.update(i,values=[(\"val_loss\",val_loss), (\"val_accuracy\",val_accuracy),(\"val_AUC_score\",val_AUC_score),(\"val_precision\",val_precision),(\"val_recall\",val_recall),(\"val_F1\",val_F1)])\n",
        "      i+=1\n",
        "\n",
        "  def predict(self, test):\n",
        "    test=self.text_process(test.text)\n",
        "    predictions = self(torch.from_numpy(test).to(torch.device(\"cuda:0\")))\n",
        "    return predictions\n",
        "\n",
        "  def save(self,PATH):  #save model weights\n",
        "    torch.save(self.state_dict(), PATH)\n",
        "\n",
        "  def load(self,PATH): #load model weights\n",
        "    model.load_state_dict(torch.load(PATH))\n",
        "    model.eval()  \n",
        "        \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLKaLRO4C2ZT",
        "colab_type": "text"
      },
      "source": [
        "# **CA BILSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQL8MUDdC45l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM_IC1_CLF(LSTM_CLF):\n",
        "    # RNN classification of the target text, with context representation concatenated.\n",
        "    # The resulting representation of the target text is concatenated with the representation\n",
        "    # of the parent text. The parent text representation comes from a single-level Bidirectional RNN.\n",
        "    # The target text representation comes from a stacked LSTM.\n",
        "\n",
        "  def __init__(self, prefix=\"IC1\", **kwargs):\n",
        "    super(LSTM_IC1_CLF, self).__init__(**kwargs)\n",
        "    self.prefix = prefix\n",
        "\n",
        "    #parent BILSTM Layer\n",
        "    self.parent_emb=nn.Embedding(self.vocab_size+2,100,padding_idx=0)\n",
        "    self.parent_encoder=nn.LSTM(100,self.hidden_size,bidirectional=True)\n",
        "    #new dense layer\n",
        "    self.dense=nn.Linear(self.hidden_size*4,self.hidden_size)\n",
        "\n",
        "  # def init_Parent_hidden(self,batch_size):\n",
        "  #       # Before we've done anything, we dont have any hidden state.\n",
        "  #       # Refer to the Pytorch documentation to see exactly\n",
        "  #       # why they have this dimensionality.\n",
        "  #       # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
        "  #       return (Variable(torch.zeros(2, batch_size, 64)).cuda(),   \n",
        "  #               Variable(torch.zeros(2, batch_size, 64).cuda()))    # <- change here: first dim of hidden needs to be doubled\n",
        "\n",
        "\n",
        "  #Forward pass\n",
        "\n",
        "  def forward(self,target,parent):\n",
        "    #take parent represantation\n",
        "    batch_size=len(target)\n",
        "    hidden=self.init_hidden(batch_size) #initialize hidden state of parent BILSTM\n",
        "    parent_embds=self.parent_emb(parent.to(torch.long))\n",
        "    parent_encoding,_=self.parent_encoder(parent_embds.view(self.max_length,batch_size,100),hidden)\n",
        "\n",
        "    #take output of last hidden state of parent encoder\n",
        "    parent=parent_encoding.squeeze()[-1,:]\n",
        "    \n",
        "    #take child represantation\n",
        "    hidden=self.init_hidden(batch_size) #initialize initial hidden state of Target BILTM\n",
        "    embeds=self.word_embeds(target.to(torch.long))\n",
        "    embeds=self.embeds_bn(embeds)\n",
        "    stacks, _ =self.stacked_lstm(embeds.view(self.max_length,batch_size,self.word_embedding_size),hidden)\n",
        "    concatenated=stacks.squeeze()[-1,:]\n",
        "    target=self.lstm_bn(concatenated)\n",
        "\n",
        "    #concatenate parent and child represantation \n",
        "    concatenated=torch.cat((parent,target),dim=1)  \n",
        "    output=self.dense(concatenated)\n",
        "    output=self.tanh(output)\n",
        "    output=self.dense1_bn(output)\n",
        "    output=self.out(output)\n",
        "    output=self.sigmoid(output)\n",
        "    #print(output)\n",
        "    return output.to(torch.float)\n",
        "\n",
        "  def text_process(self, texts, parents):\n",
        "        target_x = self.tokenizer.texts_to_sequences(texts.to_numpy())\n",
        "        target_x = sequence.pad_sequences(target_x, maxlen=self.max_length)  # padding\n",
        "        parent_x = self.tokenizer.texts_to_sequences(parents.to_numpy())\n",
        "        parent_x = sequence.pad_sequences(parent_x, maxlen=self.max_length)  # padding\n",
        "        #return [target_x, parent_x]\n",
        "        return torch.from_numpy(target_x),torch.from_numpy(parent_x)\n",
        "  \n",
        "\n",
        "  \n",
        "  def trainin(self,X_train_target,X_train_parent,Y_train, optimizer,device):\n",
        "    train_ds = TensorDataset(X_train_target,X_train_parent,Y_train)\n",
        "    train_dl = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n",
        "    epoch_loss=0\n",
        "    epoch_Auc=0\n",
        "    epoch_accuracy=0\n",
        "    epoch_recall=0\n",
        "    epoch_precision=0\n",
        "    epoch_F1=0\n",
        "    Y=torch.empty(0) #create empty torch to append predictions\n",
        "    self.train()\n",
        "    for xb,xp ,yb in train_dl:\n",
        "        xb, xp,yb = xb.to(device),xp.to(device), yb.to(device)  #move batches to GPU or CPU\n",
        "        # self.zero_grad() #DES TO\n",
        "        predictions =self(xb,xp)  #run model for mini batch\n",
        "        predictions=predictions.squeeze()\n",
        "        loss=self.loss(predictions,yb.to(torch.float))#self.loss(predictions,yb.to(torch.float))#.view(len(xb),1)) #compute loss of mini batch\n",
        "        epoch_loss+=loss.item()\n",
        "        Y=torch.cat((Y,predictions.cpu()),0)\n",
        "        loss.backward() #compute gradients\n",
        "        nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
        "        optimizer.step() #update parameters\n",
        "        optimizer.zero_grad() \n",
        "        #epoch_loss.detach().item()\n",
        "    with torch.no_grad():\n",
        "      predictions=Y\n",
        "      rounded=torch.round(predictions).squeeze().cpu().detach().numpy()\n",
        "      predictions=predictions.cpu().detach().numpy()\n",
        "      epoch_Auc=roc_auc_score(Y_train.cpu().numpy(),predictions)\n",
        "      epoch_accuracy=accuracy_score(Y_train.cpu().numpy(),rounded)\n",
        "      tn, fp, fn, tp =confusion_matrix(Y_train.cpu().numpy(),rounded,labels=[0,1]).ravel()\n",
        "      epoch_precision+=tp/(tp+fp)\n",
        "      epoch_recall+=tp/(tp+fn)\n",
        "      epoch_F1+=2*((epoch_precision*epoch_recall)/(epoch_precision+epoch_recall))\n",
        "    return epoch_loss,epoch_Auc,epoch_accuracy,epoch_recall,epoch_precision,epoch_F1\n",
        "    #return epoch_loss,epoch_Auc,epoch_accuracy\n",
        "\n",
        "\n",
        "\n",
        "  def evaluate(self,X_dev_target,X_dev_parent,Y_dev,device):\n",
        "    dev_ds = TensorDataset(X_dev_target,X_dev_parent,Y_dev)\n",
        "    dev_dl = DataLoader(dev_ds, batch_size=self.batch_size)\n",
        "    epoch_loss=0\n",
        "    epoch_recall=0\n",
        "    epoch_precision=0\n",
        "    epoch_F1=0\n",
        "    Y=torch.empty(0) #create empty torch to append predictions\n",
        "    self.eval()\n",
        "    with torch.no_grad():   # compute validation loss\n",
        "      for xb, xp,yb in dev_dl:\n",
        "        xb,xp,yb=xb.to(device), xp.to(device) ,yb.to(device) #move to Gpu\n",
        "        predictions =self(xb,xp)\n",
        "        Y=torch.cat((Y,predictions.cpu()),0)\n",
        "        val_loss=self.loss(predictions.squeeze(), yb.to(torch.float))\n",
        "        epoch_loss+=val_loss\n",
        "      predictions=Y\n",
        "      # predictions=self(X_dev_target.to(device),X_dev_parent.to(device))\n",
        "      rounded=torch.round(predictions).squeeze().cpu().detach().numpy()\n",
        "      predictions=predictions.cpu().detach().numpy()\n",
        "      epoch_Auc=roc_auc_score(Y_dev.cpu().numpy(),predictions)\n",
        "      epoch_accuracy=accuracy_score(Y_dev.cpu().numpy(),rounded)\n",
        "      tn, fp, fn, tp =confusion_matrix(Y_dev.cpu().numpy(),rounded,labels=[0,1]).ravel()\n",
        "      epoch_precision+=tp/(tp+fp)\n",
        "      epoch_recall+=tp/(tp+fn)\n",
        "      epoch_F1+=2*((epoch_precision*epoch_recall)/(epoch_precision+epoch_recall))\n",
        "      return epoch_loss,epoch_Auc,epoch_accuracy,epoch_recall,epoch_precision,epoch_F1\n",
        "      #print(epoch_loss,epoch_Auc,epoch_accuracy)\n",
        "      #return epoch_loss,epoch_Auc,epoch_accuracy\n",
        "\n",
        "\n",
        "\n",
        "  def fit(self, train, dev,optimizer, pretrained_embeddings, class_weights={0: 1, 1: 1}):\n",
        "    texts = train.text if not self.augmented_vocabulary else train.text + train.parent\n",
        "    self.tokenizer.fit_on_texts(texts)\n",
        "    self.vocab_size = len(self.tokenizer.word_index) + 1\n",
        "    print('Vocabulary Size: %d' % self.vocab_size)\n",
        "    X_train_target,X_train_parent=self.text_process(train.text,train.parent)\n",
        "    Y_train=torch.from_numpy(train.label.to_numpy())\n",
        "    X_dev_target,X_dev_parent=self.text_process(dev.text,dev.parent)\n",
        "    Y_dev=torch.from_numpy(dev.label.to_numpy())\n",
        "    early_stopping = EarlyStopping(patience=self.patience, verbose=True)     #EARLYY STOPING\n",
        "    self.load_embeddings(pretrained_embeddings)\n",
        "    pos = sum(train.label)\n",
        "    neg = len(train.label)-pos\n",
        "    bias = np.log(pos/neg)\n",
        "    print(bias,\"BIASSSS\")\n",
        "    bias=torch.tensor(bias)\n",
        "    #self.init_weights(bias.to(torch.float))\n",
        "    with torch.no_grad():\n",
        "      self.out.bias=torch.nn.Parameter(bias.to(torch.float)) # set bias of last dense layer log(T/N)\n",
        "    self.out.bias.requires_grad_(False)\n",
        "    i=1 #for progress bar\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
        "      print(\"Running on the GPU\")\n",
        "    else:\n",
        "      device = torch.device(\"cpu\")\n",
        "      print(\"Running on the CPU\")\n",
        "    self.to(device)\n",
        "    for epoch in range(self.n_epochs):\n",
        "      print('\\nEpoch: %d/%d' % (epoch + 1, self.n_epochs))\n",
        "      kbar = pkbar.Kbar(target=self.n_epochs, width=10)\n",
        "      kbar_val=pkbar.Kbar(target=self.n_epochs, width=10)\n",
        "      epoch_loss,epoch_Auc,epoch_accuracy,epoch_recall,epoch_precision,epoch_F1=self.trainin(X_train_target,X_train_parent,Y_train,optimizer,device)\n",
        "      kbar.update(i, values=[(\"loss\",epoch_loss), (\"accuracy\",epoch_accuracy),(\"AUC_score\",epoch_Auc),(\"precision\",epoch_precision),(\"recall\",epoch_recall),(\"F1\",epoch_F1)])\n",
        "      val_loss,val_AUC_score,val_accuracy,val_recall,val_precision,val_F1=self.evaluate(X_dev_target,X_dev_parent,Y_dev,device)\n",
        "      print(\"Val auc score in epoch \",epoch+1, \":\",val_AUC_score)\n",
        "      early_stopping(val_AUC_score,self)   \n",
        "      if early_stopping.early_stop: #check for early stopping\n",
        "        print(\"Early stopping\")\n",
        "        print(\"_________________________________________________-\")\n",
        "        break\n",
        "      kbar_val.update(i,values=[(\"val_loss\",val_loss), (\"val_accuracy\",val_accuracy),(\"val_AUC_score\",val_AUC_score),(\"val_precision\",val_precision),(\"val_recall\",val_recall),(\"val_F1\",val_F1)])\n",
        "      i+=1\n",
        "\n",
        "  def predict(self, test):\n",
        "    test_target,test_parent=self.text_process(test.text,test.parent)\n",
        "    predictions = self(test_target.to(torch.device(\"cuda:0\")),test_parent.to(torch.device(\"cuda:0\")))\n",
        "    return predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eduk0NjTuP4q",
        "colab_type": "text"
      },
      "source": [
        "# **Monte Carlo Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ16UsAMugdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MC_Validation(dataset,k=5):\n",
        "  avgscore=0\n",
        "  for i in range(k):\n",
        "    model= LSTM_CLF(vocab_size=size,n_epochs=20)\n",
        "    #model=LSTM_IC1_CLF(vocab_size=size,n_epochs=10)\n",
        "    optimizer=optim.Adam(model.parameters(), lr=1e-03)\n",
        "\n",
        "    # train @N models\n",
        "    X_train, X_test = train_test_split(dataset, test_size=0.4,random_state=i)\n",
        "    #X_train, X_val = train_test_split(X_train,test_size=0.25,random_state=i) # 0.25 x 0.8 = 0.2\n",
        "    XC_train,X_test=train_test_split(dataC, test_size=0.2,random_state=i) # test dataset with C to train and test\n",
        "    XC_train,X_val= train_test_split(XC_train,test_size=0.25,random_state=i) # 0.25 x 0.8 = 0.2\n",
        "\n",
        "    # train @C models\n",
        "    # X_train, X_test = train_test_split(dataset, test_size=0.4,random_state=i)\n",
        "    # #X_train, X_val = train_test_split(X_train,test_size=0.25,random_state=i) # 0.25 x 0.8 = 0.2\n",
        "    # X_val,X_test=train_test_split(X_test, test_size=0.5,random_state=i) # test dataset with C to train and test\n",
        "\n",
        "    # Perform MC Validation\n",
        "    model.fit(X_train,X_val,optimizer,pretrained_embeddings=embeddings)  #bert_weights=\"bert_weights.h5\"\n",
        "    preds=model.predict(X_test)#X_test.text.to_numpy())\n",
        "    preds=preds.squeeze().cpu().detach().numpy()\n",
        "    gold=X_test.label\n",
        "    print(\"\\n__________________________________\\n\") \n",
        "    score = roc_auc_score(gold, preds)\n",
        "    print(\"AUC score in \",i+1,\" fold \",score)\n",
        "    print(\"\\n__________________________________\") #to see results\n",
        "    avgscore+=score\n",
        "  avgscore/=k\n",
        "  return avgscore\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykzw9FGwMZYQ",
        "colab_type": "text"
      },
      "source": [
        "## **Load Dataset and Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIfg0HQpGQ5R",
        "colab_type": "code",
        "outputId": "be6705aa-0830-4c77-d4a8-8d58d49247b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataN=pd.read_csv(\"dataset/oc.csv\",header=0)\n",
        "dataC=pd.read_csv(\"dataset/wc.csv\",header=0)\n",
        "size=Compute_Vocab_Size(dataN)\n",
        "embeddings=load_embeddings_index()\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 21658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgdbhXlqWdKy",
        "colab_type": "text"
      },
      "source": [
        "# Seperate toxic comments for the rest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFcb8e3K_oCx",
        "colab_type": "code",
        "outputId": "d5503ae5-da87-4b72-b02c-4559119be4f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Get toxic comments and give them always to train set\n",
        "toxics=dataN.loc[dataN['label']==1]\n",
        "Non_toxics=dataN[dataN.label != 1]\n",
        "print(Non_toxics.shape)\n",
        "# Non_toxics=dataN.loc[dataN['label']==0]\n",
        "#Non_toxics=Non_toxics[0:59]\n",
        "# #print(Non_toxics.label.head(100))\n",
        "# frames = [toxics,Non_toxics]\n",
        "# data = pd.concat(frames)\n",
        "#print(data.shape)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9941, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl1PLoHbSWt9",
        "colab_type": "text"
      },
      "source": [
        "# Run 5-fold MC Validation(Make sure all toxic comments are always in training set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2b_IBrjWcMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# toxics=dataN.loc[dataN['label']==1]\n",
        "# #toxics=toxics[0:5]\n",
        "# #print(toxics.label.head(100))\n",
        "# # print(toxics.label.head(5))\n",
        "# Non_toxics=dataN.loc[dataN['label']==0]\n",
        "# Non_toxics=Non_toxics[0:59]\n",
        "# #print(Non_toxics.label.head(100))\n",
        "# frames = [toxics,Non_toxics]\n",
        "# data = pd.concat(frames)\n",
        "# avgscore=0\n",
        "# print(data.shape)\n",
        "\n",
        "\n",
        "#Make 5-fold Monte carlo cross Validation (Train set has always all the toxic comments of @N dataset)\n",
        "\n",
        "avgscore=0\n",
        "for i in range(5):\n",
        "  model= LSTM_CLF(vocab_size=size,n_epochs=20)\n",
        "  #model=LSTM_IC1_CLF(vocab_size=size,n_epochs=10)\n",
        "  optimizer=optim.Adam(model.parameters(), lr=1e-03)\n",
        "  # train @N models\n",
        "  X_train, X_val = train_test_split(Non_toxics, test_size=0.4023,random_state=0)\n",
        "  #Give to train set all toxic comments\n",
        "  frames = [toxics,X_train]\n",
        "  X_train = pd.concat(frames)\n",
        "  XC_train,X_test=train_test_split(dataC, test_size=0.2,random_state=i) # test dataset with C to train and test\n",
        "  XC_train,X_val= train_test_split(XC_train,test_size=0.25,random_state=i) # 0.25 x 0.8 = 0.2\n",
        "  model.fit(X_train,X_val,optimizer,pretrained_embeddings=embeddings)  #bert_weights=\"bert_weights.h5\"\n",
        "  preds=model.predict(X_test)\n",
        "  preds=preds.squeeze().cpu().detach().numpy()\n",
        "  #preds=torch.round(preds).squeeze().cpu().detach().numpy()\n",
        "  gold=X_test.label\n",
        "  print(\"\\n__________________________________\\n\") \n",
        "  score = roc_auc_score(gold, preds)\n",
        "  print(\"AUC score in \",i+1,\" fold \",score)\n",
        "  print(\"\\n__________________________________\") #to see results\n",
        "  avgscore+=score\n",
        "result=avgscore/5\n",
        "print(\"Average AUC score over 5fold MC validation is \",result) \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riodOvPaGi6G",
        "colab_type": "text"
      },
      "source": [
        "# **Stratified Split And MC(5-fold)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5y4ZVN5Dhecp",
        "outputId": "c64ad4ce-ff66-48d2-ab48-fb231972388d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Make stratified split in @N dataset for train data\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.4)   #random_state=0)\n",
        "X=dataN.text.to_numpy()\n",
        "y=dataN.label.to_numpy()\n",
        "train=list(sss.split(X,y))\n",
        "\n",
        "#Make stratified split in @C dataset for val and test data\n",
        "sss_Val = StratifiedShuffleSplit(n_splits=1, test_size=0.4)    #random_state=0)\n",
        "X_val=dataC.text.to_numpy()\n",
        "y_val=dataC.label.to_numpy()\n",
        "val_and_test=list(sss_Val.split(X_val,y_val))\n",
        "train_index,test_index = val_and_test[0]\n",
        "X_train, X_val_test = X_val[train_index], X_val[test_index]\n",
        "y_train, y_val_test = y_val[train_index], y_val[test_index]\n",
        "\n",
        "# Now make stratified split in 40% of C dataset for val(20%) and test(20%)\n",
        "sss_Val= StratifiedShuffleSplit(n_splits=5, test_size=0.5)#, random_state=0)\n",
        "val_and_test=list( sss_Val.split(X_val_test,y_val_test))\n",
        "avgscore=0\n",
        "for i in range(5):\n",
        "  train_index, _ = train[i]\n",
        "  val_index , test_index = val_and_test[i]\n",
        "  X_train,y_train=X[train_index],y[train_index]\n",
        "  X_val,y_val=X_val_test[val_index],y_val_test[val_index]\n",
        "  X_test,y_test=X_val_test[test_index],y_val_test[test_index]\n",
        "  \n",
        "  # Create X_train , X_val and X_test Dataframes\n",
        "  X_train=pd.DataFrame({'text': X_train,'label': y_train})\n",
        "  X_val=pd.DataFrame({'text': X_val,'label': y_val})\n",
        "  X_test=pd.DataFrame({'text': X_test,'label': y_test})\n",
        "  print(X_train.loc[X_train['label']==1].shape)\n",
        "  # print(X_val.shape)\n",
        "  # print(X_test.shape)\n",
        "  model= LSTM_CLF(vocab_size=size,n_epochs=50)\n",
        "  #model=LSTM_IC1_CLF(vocab_size=size,n_epochs=10)\n",
        "  optimizer=optim.Adam(model.parameters(), lr=1e-03)\n",
        "  model.fit(X_train,X_val,optimizer,pretrained_embeddings=embeddings)  #bert_weights=\"bert_weights.h5\"\n",
        "  preds=model.predict(X_test)\n",
        "  preds=preds.squeeze().cpu().detach().numpy()\n",
        "  #preds=torch.round(preds).squeeze().cpu().detach().numpy()\n",
        "  gold=X_test.label.to_numpy()\n",
        "  print(\"\\n__________________________________\\n\") \n",
        "  score = roc_auc_score(gold, preds)\n",
        "  print(\"AUC score in \",i+1,\" fold \",score)\n",
        "  print(\"\\n__________________________________\") #to see results\n",
        "  avgscore+=score\n",
        "result=avgscore/5\n",
        "print(\"Average AUC score over 5fold MC validation is \",result) \n",
        "  \n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35, 2)\n",
            "Vocabulary Size: 15790\n",
            "-5.138316273042602 BIASSSS\n",
            "Running on the GPU\n",
            "\n",
            "Epoch: 1/50\n",
            " 1/50 [..........] - ETA: 2:25 - loss: 28.5768 - accuracy: 0.3678 - AUC_score: 0.5649 - precision: 0.0071 - recall: 0.7714 - F1: 0.0140"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  1 : 0.4764720812182741\n",
            "Validation loss decreased (inf --> 0.476472).  Saving model ...\n",
            "\r 1/50 [..........] - ETA: 2:49 - val_loss: 3.5933 - val_accuracy: 0.0150 - val_AUC_score: 0.4765 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 2/50\n",
            " 2/50 [..........] - ETA: 1:03 - loss: 8.1823 - accuracy: 0.0058 - AUC_score: 0.4939 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  2 : 0.4745769881556684\n",
            "EarlyStopping counter: 1 out of 3\n",
            "\r 2/50 [..........] - ETA: 1:14 - val_loss: 3.3387 - val_accuracy: 0.0150 - val_AUC_score: 0.4746 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 3/50\n",
            " 3/50 [..........] - ETA: 42s - loss: 8.1515 - accuracy: 0.0058 - AUC_score: 0.5333 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  3 : 0.47117597292724195\n",
            "EarlyStopping counter: 2 out of 3\n",
            "\r 3/50 [..........] - ETA: 49s - val_loss: 3.1029 - val_accuracy: 0.0150 - val_AUC_score: 0.4712 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 4/50\n",
            " 4/50 [..........] - ETA: 31s - loss: 8.1568 - accuracy: 0.0058 - AUC_score: 0.5066 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  4 : 0.45671742808798654\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            "_________________________________________________-\n",
            "\n",
            "__________________________________\n",
            "\n",
            "AUC score in  1  fold  0.41884940778341795\n",
            "\n",
            "__________________________________\n",
            "(35, 2)\n",
            "Vocabulary Size: 15884\n",
            "-5.138316273042602 BIASSSS\n",
            "Running on the GPU\n",
            "\n",
            "Epoch: 1/50\n",
            " 1/50 [..........] - ETA: 2:12 - loss: 29.5088 - accuracy: 0.3643 - AUC_score: 0.4831 - precision: 0.0058 - recall: 0.6286 - F1: 0.0114"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  1 : 0.5515736040609137\n",
            "Validation loss decreased (inf --> 0.551574).  Saving model ...\n",
            "\r 1/50 [..........] - ETA: 2:35 - val_loss: 3.6000 - val_accuracy: 0.0150 - val_AUC_score: 0.5516 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 2/50\n",
            " 2/50 [..........] - ETA: 1:04 - loss: 8.1658 - accuracy: 0.0058 - AUC_score: 0.4874 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  2 : 0.5380033840947547\n",
            "EarlyStopping counter: 1 out of 3\n",
            "\r 2/50 [..........] - ETA: 1:15 - val_loss: 3.4243 - val_accuracy: 0.0150 - val_AUC_score: 0.5380 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 3/50\n",
            " 3/50 [..........] - ETA: 42s - loss: 8.1741 - accuracy: 0.0058 - AUC_score: 0.4652 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  3 : 0.5375042301184433\n",
            "EarlyStopping counter: 2 out of 3\n",
            "\r 3/50 [..........] - ETA: 49s - val_loss: 3.1733 - val_accuracy: 0.0150 - val_AUC_score: 0.5375 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 4/50\n",
            " 4/50 [..........] - ETA: 31s - loss: 8.1650 - accuracy: 0.0058 - AUC_score: 0.5087 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  4 : 0.534864636209814\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            "_________________________________________________-\n",
            "\n",
            "__________________________________\n",
            "\n",
            "AUC score in  2  fold  0.43263113367174283\n",
            "\n",
            "__________________________________\n",
            "(35, 2)\n",
            "Vocabulary Size: 15803\n",
            "-5.138316273042602 BIASSSS\n",
            "Running on the GPU\n",
            "\n",
            "Epoch: 1/50\n",
            " 1/50 [..........] - ETA: 2:11 - loss: 28.4763 - accuracy: 0.3460 - AUC_score: 0.4788 - precision: 0.0061 - recall: 0.6857 - F1: 0.0121"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  1 : 0.5150084602368866\n",
            "Validation loss decreased (inf --> 0.515008).  Saving model ...\n",
            "\r 1/50 [..........] - ETA: 2:35 - val_loss: 3.3635 - val_accuracy: 0.0150 - val_AUC_score: 0.5150 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 2/50\n",
            " 2/50 [..........] - ETA: 1:05 - loss: 8.1730 - accuracy: 0.0058 - AUC_score: 0.5349 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  2 : 0.5090439932318105\n",
            "EarlyStopping counter: 1 out of 3\n",
            "\r 2/50 [..........] - ETA: 1:15 - val_loss: 3.2404 - val_accuracy: 0.0150 - val_AUC_score: 0.5090 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 3/50\n",
            " 3/50 [..........] - ETA: 42s - loss: 8.1670 - accuracy: 0.0058 - AUC_score: 0.4759 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  3 : 0.5110321489001692\n",
            "EarlyStopping counter: 2 out of 3\n",
            "\r 3/50 [..........] - ETA: 49s - val_loss: 3.4383 - val_accuracy: 0.0150 - val_AUC_score: 0.5110 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 4/50\n",
            " 4/50 [..........] - ETA: 30s - loss: 8.1610 - accuracy: 0.0058 - AUC_score: 0.5161 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  4 : 0.5091624365482234\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            "_________________________________________________-\n",
            "\n",
            "__________________________________\n",
            "\n",
            "AUC score in  3  fold  0.526497461928934\n",
            "\n",
            "__________________________________\n",
            "(35, 2)\n",
            "Vocabulary Size: 15680\n",
            "-5.138316273042602 BIASSSS\n",
            "Running on the GPU\n",
            "\n",
            "Epoch: 1/50\n",
            " 1/50 [..........] - ETA: 2:10 - loss: 29.0758 - accuracy: 0.3473 - AUC_score: 0.5930 - precision: 0.0071 - recall: 0.8000 - F1: 0.0141"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  1 : 0.48771573604060914\n",
            "Validation loss decreased (inf --> 0.487716).  Saving model ...\n",
            "\r 1/50 [..........] - ETA: 2:35 - val_loss: 3.2882 - val_accuracy: 0.0150 - val_AUC_score: 0.4877 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 2/50\n",
            " 2/50 [..........] - ETA: 1:04 - loss: 8.1579 - accuracy: 0.0058 - AUC_score: 0.5249 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  2 : 0.4811082910321489\n",
            "EarlyStopping counter: 1 out of 3\n",
            "\r 2/50 [..........] - ETA: 1:15 - val_loss: 3.3082 - val_accuracy: 0.0150 - val_AUC_score: 0.4811 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 3/50\n",
            " 3/50 [..........] - ETA: 42s - loss: 8.1553 - accuracy: 0.0058 - AUC_score: 0.4220 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  3 : 0.47976311336717425\n",
            "EarlyStopping counter: 2 out of 3\n",
            "\r 3/50 [..........] - ETA: 49s - val_loss: 3.3331 - val_accuracy: 0.0150 - val_AUC_score: 0.4798 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 4/50\n",
            " 4/50 [..........] - ETA: 30s - loss: 8.1625 - accuracy: 0.0058 - AUC_score: 0.4508 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  4 : 0.48019458544839255\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            "_________________________________________________-\n",
            "\n",
            "__________________________________\n",
            "\n",
            "AUC score in  4  fold  0.5151184433164129\n",
            "\n",
            "__________________________________\n",
            "(35, 2)\n",
            "Vocabulary Size: 15809\n",
            "-5.138316273042602 BIASSSS\n",
            "Running on the GPU\n",
            "\n",
            "Epoch: 1/50\n",
            " 1/50 [..........] - ETA: 2:13 - loss: 29.4926 - accuracy: 0.3735 - AUC_score: 0.5759 - precision: 0.0074 - recall: 0.8000 - F1: 0.0147"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  1 : 0.4775380710659898\n",
            "Validation loss decreased (inf --> 0.477538).  Saving model ...\n",
            "\r 1/50 [..........] - ETA: 2:36 - val_loss: 3.6346 - val_accuracy: 0.0150 - val_AUC_score: 0.4775 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 2/50\n",
            " 2/50 [..........] - ETA: 1:04 - loss: 8.1734 - accuracy: 0.0058 - AUC_score: 0.5042 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  2 : 0.4818104906937394\n",
            "Validation loss decreased (0.477538 --> 0.481810).  Saving model ...\n",
            "\r 2/50 [..........] - ETA: 1:16 - val_loss: 3.2420 - val_accuracy: 0.0150 - val_AUC_score: 0.4818 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 3/50\n",
            " 3/50 [..........] - ETA: 42s - loss: 8.1571 - accuracy: 0.0058 - AUC_score: 0.5026 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  3 : 0.4836548223350254\n",
            "Validation loss decreased (0.481810 --> 0.483655).  Saving model ...\n",
            "\r 3/50 [..........] - ETA: 50s - val_loss: 3.2698 - val_accuracy: 0.0150 - val_AUC_score: 0.4837 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 4/50\n",
            " 4/50 [..........] - ETA: 30s - loss: 8.1715 - accuracy: 0.0058 - AUC_score: 0.4857 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  4 : 0.4929949238578679\n",
            "Validation loss decreased (0.483655 --> 0.492995).  Saving model ...\n",
            "\r 4/50 [..........] - ETA: 36s - val_loss: 3.2896 - val_accuracy: 0.0150 - val_AUC_score: 0.4930 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 5/50\n",
            " 5/50 [>.........] - ETA: 24s - loss: 8.1692 - accuracy: 0.0058 - AUC_score: 0.4780 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  5 : 0.48606598984771565\n",
            "EarlyStopping counter: 1 out of 3\n",
            "\r 5/50 [>.........] - ETA: 28s - val_loss: 3.1060 - val_accuracy: 0.0150 - val_AUC_score: 0.4861 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 6/50\n",
            " 6/50 [>.........] - ETA: 20s - loss: 8.1903 - accuracy: 0.0058 - AUC_score: 0.5662 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  6 : 0.4859898477157361\n",
            "EarlyStopping counter: 2 out of 3\n",
            "\r 6/50 [>.........] - ETA: 23s - val_loss: 3.1543 - val_accuracy: 0.0150 - val_AUC_score: 0.4860 - val_precision: 0.0150 - val_recall: 1.0000 - val_F1: 0.0296"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 7/50\n",
            " 7/50 [>.........] - ETA: 16s - loss: 8.1585 - accuracy: 0.0058 - AUC_score: 0.4947 - precision: 0.0058 - recall: 1.0000 - F1: 0.0116"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([80, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val auc score in epoch  7 : 0.4873181049069374\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            "_________________________________________________-\n",
            "\n",
            "__________________________________\n",
            "\n",
            "AUC score in  5  fold  0.5395854483925551\n",
            "\n",
            "__________________________________\n",
            "Average AUC score over 5fold MC validation is  0.48653637901861246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy-_J-txSoP1",
        "colab_type": "text"
      },
      "source": [
        "# Create a balanced small dataset (with all toxic comments and try to overfit on it )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K4oHvkwSfje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get toxic comments and give them always to train set\n",
        "toxics=dataN.loc[dataN['label']==1]\n",
        "Non_toxics=dataN[dataN.label != 1]\n",
        "print(Non_toxics.shape)\n",
        "# Non_toxics=dataN.loc[dataN['label']==0]\n",
        "Non_toxics=Non_toxics[0:59]\n",
        "# #print(Non_toxics.label.head(100))\n",
        "frames = [toxics,Non_toxics]\n",
        "data = pd.concat(frames)\n",
        "print(data.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "avgscore=0\n",
        "for i in range(5):\n",
        "  model= LSTM_CLF(vocab_size=size,n_epochs=20)\n",
        "  #model=LSTM_IC1_CLF(vocab_size=size,n_epochs=10)\n",
        "  optimizer=optim.Adam(model.parameters(), lr=1e-03)\n",
        "  X_train, X_val = train_test_split(data, test_size=0.2,random_state=i)\n",
        "  model.fit(data,X_val,optimizer,pretrained_embeddings=embeddings)  #bert_weights=\"bert_weights.h5\"\n",
        "  preds=model.predict(data)\n",
        "  preds=preds.cpu().detach().numpy()\n",
        "  gold=data.label.to_numpy()\n",
        "  print(\"\\n__________________________________\\n\") \n",
        "  score = roc_auc_score(gold, preds)\n",
        "  print(\"AUC score in \",i+1,\" fold \",score)\n",
        "  print(\"\\n__________________________________\") #to see results\n",
        "  avgscore+=score\n",
        "result=avgscore/5\n",
        "print(\"Average AUC score over 5fold MC validation is \",result) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH1IckouUaRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Average score after 5 MC is \",MC_Validation(dataset=dataN,k=5))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}